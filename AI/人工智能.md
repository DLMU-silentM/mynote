# 人工智能考纲

<!-- [TOC] -->

## 1.人工智能的概念，人工智能研究的基本内容，人工智能研究的主要学派及其观点，以及人工智能应用概况。

### 人工智能的概念：

- **形式化定义**：目前还没有。因为人工智能的严格定义依赖于对智能的定义，而智能本身也还无严格定义。
  
  - **智能**： 人类在认识和改造世界的活动中，由脑力劳动表现出来的能力。包括感知、理解、抽象、分析、推理、判断、学习和对变化环境的适应等等

- **一般解释**：人工智能就是用人工的方法在机器（计算机）上实现的智能，或称机器智能
  
  - **人工智能学科**：从学科的角度来说，人工智能是一门研究如何构造智能机器或智能系统，使之能模拟、延伸、扩展人类智能的学科

- **人工智能能力**：从智能能力的角度来说，人工智能是智能机器所执行的通常与人类智能有关的智能行为，如判断、推理、证明、识别、感知、理解、通信、设计、思考、规划、学习和问题求解等思维活动。

- ***书本上**：*
  
  - 类人行为方法
    - 人工智能是一种创建机器的技艺，这种机器能够执行需要人的智能才能完成的功能[Kurzweil 1990]。
  - 类人思维方法
    - 人工智能是那些与人的思维、决策、问题求解和学习等有关活动的自动化[Bellman 1978]。
  - 理性思维方法
    - 用计算模型研究智力能力[Charniak et al.1985]。
  - 理性行为方法
    - 人工智能关心的是人工制品中的智能行为[Nilsson 1998]。
  - <mark>人工智能主要研究用人工的方法和技术，模仿、延伸和扩展人的智能，实现机器智能。</mark>人工智能的长期目标是实现达到人类智力水平的人工智能。

### 人工智能研究的基本内容：

- ⑴ **知识表示** 将人类的知识形式化或模型化。
- ⑵ **机器感知** 使机器具有类似人的感知能力。
- ⑶ **机器学习** 使机器具有获取新知识、学习新技巧，并在实践中不断完善、改进的能力。
- ⑷ **机器思维** 对感知到的外部信息及机器内部的各种工作信息进行有目的的处理。
- ⑸ **机器行为** 使机器具有与人的行为能力相应的能力。
- ⑹ **系统构成** 研究构建智能系统或智能机器的模型、系统分析、构造技术、建造工具、语言等相关技术的研究。
- ***书本上**：<mark>知识表示，搜索技术，机器学习，求解数据和知识不确定问题的各种方法。</mark>*

### 人工智能研究的主要学派及其观点：

- <mark>**符号主义（Symbolicism）**</mark>: 基于物理符号系统假设和有限合理性原理
  
  - 智能的基础是知识，其核心是知识表示和知识推理；知识可用符号表示，也可用符号进行推理，因而可以建立基于知识的人类智能和机器智能的统一的理论体系。

- <mark>**连接主义（Connectionism）**</mark>: 基于神经网络及其间的连接机制与学习算法
  
  - 思维的基元是神经元，而不是符号；思维过程是神经元的联结活动过程，而不是符号运算过程；反对符号主义关于物理符号系统的假设。

- <mark>**行为主义（Actionism）**</mark>: 基于控制论及感知—动作型控制系统
  
  - 智能取决于感知和行动，提出了智能行为的“感知—动作”模型；智能不需要知识、不需要表示、不需要推理；人工智能可以像人类智能那样逐步进化。
  - 布鲁克基于控制论提出了无需知识表示的智能、无需推理的智能。他认为智能只是在与环境的交互作用中表现出来，在许多方面是行为心理学观点在现代人工智能中的反映。

### 人工智能应用概况

- 基本技术：
  - ⑴ **知识表示技术**：将知识表示为计算机可以识别和使用的形式；
  - ⑵ **搜索推理技术**：利用已有的知识解决进行问题求解；
    - 主要包括**搜索技术**、**推理技术**、**归纳技术**、**联想技术**等四大类
    - **搜索**：为了达到某种目标而连续进行的推理过程。搜索技术就是对推理进行控制和引导的技术，它也是人工智能的基本技术之一。
      - **盲目搜索**：在搜索中不改变搜索策略，不利用搜索过程中所获得的中间信息，因而盲目性大、效率较差。
      - **启发式搜索**：在搜索过程中加入了与问题有关的启发式信息，用以指导搜索在一个比较小的范围内进行，从而加速获得结果的过程。
    - **推理**：思维的规范，推理是思维的法则。要进行智能模拟，必须使得计算机能够推理。
      - **经典逻辑**：指为适合经典数学建立可靠的基础的需要而建立的现代逻辑学，主要内容为命题逻辑和一阶谓词逻辑。
      - **非经典逻辑**：具有不同于经典逻辑的特征的现代逻辑系统的总称，主要是在20世纪20－30年代以后为了克服经典逻辑的局限性而产生的。
      - 实现计算机推理需要构造并实现推理的方法和控制策略，如**归结反演**、**自然推导法**等等。
    - **归纳**：计算机自动提取概念、抽取知识、寻找规律的技术。使计算机具有智能的根本途径。
      - 主要包括**基于符号处理和基于神经网络的归纳技术**以及**基于数据库的数据挖掘和知识发现技术**。
    - **联想**：最基本、最基础的思维活动，它几乎与所有的人工智能技术息息相关。根据目前已有的大量信息、寻找并发现对问题求解有用的信息是十分重要的。
  - ⑶ **系统构成技术**：研究如何将有关的知识、部件组织成一个高效的问题求解系统，以便在计算机中实现问题的解决。
- 研究领域：
  - **问题求解**
  - **机器学习**
  - **自然语言理解**
  - **专家系统**
  - **模式识别**
  - **计算机视觉**
  - **机器人学**
  - **博弈**
  - **计算智能**
  - **人工生命**
  - **自动定理证明**
  - **自动程序设计**
  - **智能控制**
  - **智能检索**
  - **智能调度与智慧**
  - **智能决策支持系统**
  - **人工神经网络**
  - **数据挖掘与知识发现**
  - ***书本上**：<mark>专家系统，数据挖掘，自然语言处理，智能机器人，模式识别，分布式人工智能，互联网智能，博弈</mark>*

---

## 2.知识的概念、分类。

### 知识的概念：

- ⑴ 知识是经过削减、塑造、解释和转换的信息。简单地说，知识是**经过加工的信息**。— Feigenbaum
- ⑵ 知识是由**特定领域的描述、关系和过程组成**的。— Bernstein
- ⑶ 知识是**事实、信念和启发式规则**。— Hayes-Roth
- 从知识库的观点看，知识是**某领域中所涉及的各有关方面的一种符号表示**。
- ***书本上**：<mark>知识是经过加工的信息，包括事实，信念和规则。</mark>*

### 知识的分类：

- 按知识所表达的内容：
  - ⑴ **事实性知识**
  - ⑵ **过程性知识**
  - ⑶ **行为性知识**
  - ⑷ **实例性知识**
  - ⑸ **类比性知识**
  - ⑹ **元知识**        
  - ⑺ **常识（类知识）**
- 按在人工智能系统中的所处理的知识的组成成分：
  - ⑴ **事实**: 是有关问题环境的一些事物的知识，常以“…是…”的形式出现。
  - ⑵ **规则**:是有关问题中与事物的行动、动作相联系的因果关系知识，是动态的，常以“如果…那么…” 形式出现。
  - ⑶ **控制**:是有关问题的求解步骤、技巧的知识。
  - ⑷ **元知识**：是有关知识的知识，是知识库中的高层知识。
  - ***书本上**：<mark>陈述性知识（事实），程序性知识（规则），控制性知识（控制）</mark>*

---

## 3.谓词逻辑知识表示方法，谓词公式转换为子句集。

### 一阶谓词表示知识：

1. 定义谓词
2. 用连结词把有关谓词连接起来形成一个谓词公式表达一个完整的意义
- ***书本上**：*
   *1. <mark>定义谓词及个体，确定每个谓词及个体的确切含义</mark>
  2. <mark>根据所要表达的事物或概念，为每个谓词中的变元赋予特定的值</mark>
  3. <mark>根据所要表达的知识的语义，用适当的连接符号将各个谓词连接起来，形成谓词公式</mark>*

### 谓词公式化成子句集的步骤：

1. 消去蕴涵连词
2. 减小否定连词的辖域
3. 约束变元标准化
4. 消去存在量词
5. 组成全称量词前缀
6. 利用等价关系把母式化为合取范式 
7. 消去全称量词。
8. 对变元更名，使不同子句中的变元不同名。
9. 消去合取连词，得到子句集。

连结词优先级：┐> ∧ > ∨ > → > ←→

*该例子疑似计算有误*：

![20220322212058-2022-03-22-21-20-59](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220322212058-2022-03-22-21-20-59.png)

---

## 4.产生式系统知识表示方法，产生式系统的组成。

### 产生式系统知识表示方法

- 产生式通常用于表示具有因果关系的知识，其**基本形式**是
  
  > P→Q  或 IF P THEN Q
- **确定性和不确定性规则知识**的产生式表示：
  - 确定性规则知识 可用前面介绍的产生式的基本形式表示即可。
  - 不确定性规则知识 用如下形式表示
    
    > P→Q （可信度） 或者 IF P THEN Q  （可信度）
- **确定性和不确定性事实性知识**的产生式表示
  - 确定性事实性知识  一般使用三元组的形式表示如下
    
    > （对象，属性，值）或 （关系，对象1，对象2）
  - 不确定性事实性知识  一般用四元组的形式表示如下
    
    > （对象，属性，值，不确定度量值）或（关系，对象1，对象2，不确定度量值）
- ***书本上**：*
  - *<mark>事实</mark>的表示：*
    - *<特性-对象-取值>（Attribute-Object-Value）或<特性-对象-取值-可信度>（Attribute-Object-Value-MYCIN）*
  - *<mark>规则</mark>的表示*
    - *IF <前提> THEN <结论>*
    - *巴科斯范式*
      
      > <规则> ::= <前提> -> <结论>
      > <前提> ::= <简单条件> | <复合条件>
      > <结论> ::= <事实> | <动作>
      > <复合条件> ::= <简单条件> AND <简单条件> [(AND <简单条件>)...] | <简单条件> OR <简单条件> [(OR <简单条件>)...]
      > <动作> ::= <动作名> [(<变元> , ...)]

### 产生式系统的组成

- 组成部分：
  
  - ⑴ **综合数据库**： 它含有与具体任务有关的信息，用于描述问题求解状态。
  - ⑵ **规则库** ：它是产生式（规则）的集合，用于描述某领域内知识。规则的作用是当综合数据库满足规则的左部时，对数据库进行规则的右部所规定的操作或运算。
  - ⑶ **控制系统** ：也称为推理机或解释器，它由控制策略组成。控制策略规定规则的应用方式（推理方式）和当有多个规则适用时应使用哪一个规则（冲突解决方式)。

- 产生式系统的分类，按规则库及综合数据库的性质及结构特征进行分类：
  
  - **可交换的产生式系统**：如果一个产生式系统对规则的使用次序是可交换的，无论先使用哪一条规则都可达到目的，即规则的使用次序是无关紧要的，就称其为可交换的产生式系统。
  
  - **可分解的产生式系统**：一个规模较大且比较复杂的问题（初始数据库）分解为分别若干个规模较小且比较简单的子问题，然后对每个子问题进行求解。
  
  - **可恢复的产生式系统**：在问题求解的过程中，既可对综合数据库添加新内容，又可删除或修改老内容的产生式系统称为可恢复的产生式系统。  

- ***书本上**：<mark>规则库，综合数据库或工作区，以及控制系统</mark>*

---

## 5.语义网络知识表示方法。

- 语义网络
  - 语义网络是通过概念及其语义关系来表达知识一种网络图。
  - 从图论的观点看，语义网络是一个“带标识的有向图”
  - 有向图的节点代表实体，表示各种事物、概念、情况、属性、状态、事件、动作等；节点还可以是一个语义子网络，形成嵌套结构。
  - 有向图的弧代表语义关系，表示它所连结的两个实体之间的语义联系，它必须带有标识。
    - 语义基元
      - 语义网络中最基本的语义单元称为语义基元，可用三元组表示为：
        
        > （结点1，弧，结点2）
    - 基本网元
      - 指一个语义基元对应的有向图
- ***书本上**：*
  - *语义网络：词法部分，结构部分，过程部分，语义部分*
  - *复杂知识的表示：引入分块语义网络*
  - *常用语义联系：*
    - *类属关系：IS-A，AKO（A Kind Of），AIO（A Instance Of）*
    - *聚集关系：Part-Of，Menber-Of*
    - *相似关系：Similar-Of*
    - *推论关系：Reasoning-Of*
    - *因果关系：Causality*
    - *占有关系：Have*
    - *组成关系：Composed-Of*
    - *时空关系*

---

## 6.框架知识表示方法。

- 框架：是人们认识事物的一种通用的数据结构形式。即当新情况发生时，人们只要把新的数据加入到该通用数据结构中便可形成一个具体的实体(类)，这样的通用数据结构就称为框架。
- 实例框架：对于一个框架，当人们把观察或认识到的具体细节填入后，就得到了该框架的一个具体实例，框架的这种具体实例被称为实例框架。
- 框架系统：在框架理论中，框架是知识的基本单位，把一组有关的框架连结起来便可形成一个框架系统。
- 框架系统推理：由框架之间的协调来完成。 
- 框架结构：
  - 每个框架都有框架名，代表某一类对象
  - 一个框架由若干个槽（项目）组成，用于表示对象的某个方面的属性
  - 有时一个槽（属性）还要从不同的侧面来描述，每个侧面可具有一个或多个值。
- 框架网络：当知识比较复杂时，往往需要通过多个框架之间的横向或纵向联系形成一种框架网络。
- 框架之间的纵向联系：是指那种具有继承关系的上下层框架之间的联系。框架之间的纵向联系通过预定义槽名AKO或ISA等来实现。
- 框架之间的横向联系：是指那种以另外一个框架名作为一个槽的槽值或侧面值所建立起来的框架之间的联系。
  - ***书本上**：*
  - *框架结构：*
    
    > FRAME <框架名>
    > 槽名~1~：侧面名~11~  值~11~
    > .............侧面名~12~  值~12~
    > ...
    > .............侧面名~1n~  值~1n~
    > ...
    > 槽名~2~：侧面名~21~  值~21~
    > .............侧面名~22~  值~22~
    > ...
    > .............侧面名~2n~  值~2n~
    > ...
    > 约束：约束条件~1~
    > ...........约束条件~2~
    > ...
    > ...........约束条件~n~
  - *框架网络*

---

## 7.搜索的概念、分类、评价标准。

### 搜索的概念：

- 给定的问题，智能系统的行为一般是找到能够达到所希望目标的动作序列，并使其所付出的代价最小、性能最好。
- 基于给定的问题，搜索就是找到智能系统的动作序列的过程。 
- 搜索算法的输入是给定的问题，输出是表示为动作序列的方案。用搜索方法求解问题就是依据给定问题的信息，寻找从初始状态到某个目标状态的具有最优费用的动作序列。一旦有了方案，就可以执行该方案所给出的动作。

### 搜索的分类：

- 按**搜索空间的表示方式或应用目的**可以将搜索分成：
- ⑴ **状态空间搜索**；
- ⑵ **问题空间搜索**；
- ⑶ **博弈空间搜索**。
- 按搜索**是否使用启发式信息**可以将搜索分成：
  - ⑴ **盲目搜索（无信息搜索）**；
  - ⑵ **启发式搜索**。
- 按搜索**实现的控制方式**可以将搜索分成：
  - ⑴ **宽度优先搜索**；
  - ⑵ **深度优先搜索**；
  - ⑶ **有序搜索**。
- 按搜索所**使用的数据结构**可以将搜索分成：
  - ⑴ **树搜索**；
  - ⑵ **一般图搜索**；
  - ⑶ **与或图搜索**。

### 评价标准：

- **完备性**：如果存在一个解答，该策略是否保证能够找到？
- **时间复杂性**：需要多长时间可以找到解答？
- **空间复杂性**：执行搜索需要多少存储空间？
- **最优性**：如果存在不同的几个解答，该策略是否可以发现最高质量的解答？

---

## 8.深度优先搜索、宽度优先搜索、迭代加深搜索。

- 如果费用函数 的计算方法仅依赖于预先对算符代价的假定而不依赖于新结点的产生所带来的信息，使得 中结点的排列方式变得无启发式信息可以利用，这种搜索策略称为**盲目搜索**或无信息搜索。

### 深度优先搜索

- 如果OPEN表为**堆栈**，则搜索算法首先扩展最新产生的（即最深的）结点。深度相等的结点可以任意排列。这种盲目搜索称为**深度优先搜索**。深度优先搜索的特点是，扩展最深的结点的结果使得搜索沿着状态空间某条单一的路径从初始结点向下进行下去，只有当搜索到达一个没有后裔的状态时，它才考虑另一条替代的路径。
- **深度优先搜索不是完备的**。如果搜索方向正确，它找到目标结点的速度是很快的，但在搜索空间无限的情形下，深度优先搜索不能保证可以找到目标结点。
- 在深度优先搜索方法中，为了防止搜索过程沿着无益的路径扩展下去，往往将深度优先法与回溯法结合使用，即给出一个结点扩展的最大深度—深度界限。任何结点如果达到了深度界限，那么都将把它们作为没有后继结点处理，并向上回溯。这种有限深度的深度优先搜索当有目标结点在深度界限内时在理论上可以找到某个目标结点，但未必是最最优的。
- 为了保证深度优先搜索的完备性以及解的最优性，可以将使用**迭代加深控制策略**，从**深度界限**为1 开始，使用深度优先搜索，当搜索失败时将深度限制加1 ，迭代进行深度优先搜索。

### 宽度优先搜索

- 如果 OPEN表为**队列**，则搜索算法以接近初始结点的程度依次扩展结点，这种盲目搜索称为**宽度优先搜索**。宽度优先搜索的特点是，逐层进行扩展结点，在对下一层的任一结点进行搜索之前，必须搜索完本层的所有结点。
- **宽度优先搜索是完备的**，即在目标结点存在的情形下，无论搜索空间是否有限，一定能够找到从初始结点到目标结点的费用最小的路径而成功结束。当然，由于宽度优先搜索可能需要扩展的结点太多，效率极差，理论上可行并非意味着在实际中也是可行的。

---

## 9.启发式搜索函数、搜索算法、可采纳性。

- 盲目搜索效率低，耗费过多的计算空间与时间。
- 进行搜索技术一般需要某些有关具体问题领域的特性的信息，称为启发信息。利用启发信息的搜索方法叫做启发式搜索方法。
- 有关具体问题领域的信息常常可以用来简化搜索。
- 一个比较灵活（但代价也较大）的利用启发信息的方法是应用某些准则来重新排列每一步OPEN 表中所有结点的顺序。
- 然后，搜索就可能沿着某个被认为是最有希望的边缘区段向外扩展。

### 搜索（估价）函数

- 应用这种排序过程，需要某些估算结点“希望”的量度，这种量度叫做**估价函数**（evaluation function）。*一般形式为：**f(x) = g(x) + h(x)**。其中，g(x)表示从初始节点S~0~到节点x的实际代价，h(x)表示从x到目标节点S~g~的最优路径的评估代价。*

### 搜索算法

- ***书本上**：搜索算法有：爬山算法，模拟退火算法，最好优先算法，通用图搜索算法，A\*算法，迭代加深A\*算法。*
- 搜索算法分类
  - **全局排序**——对OPEN表中的所有节点排序，使最有希望的节点排在表首。
    - **A算法**， **A*算法**
  - **局部排序**——仅对新扩展出来的子节点排序，使这些新节点中最有希望者能优先取出考察和扩展；
    - **爬山法**

### 可采纳性：

- 在存在从初始状态节点到目标状态节点解答路径的情况下，若一个搜索法总能找到最短（代价最小）的解答路径，则称该状态空间中的搜索算法具有可采纳性，也叫最优性。

---

## 10.爬山算法、模拟退火算法、最好优先算法、A*算法。

### 爬山算法

- 爬山算法采用了前面定义的评估函数f（x）来估计目标状态和当前状态的“距离”。当一个节点被扩展以后，对节点x进行评估得到f（x），按f（x）的升序排列这些函数值，并把这些节点按f（x）的升序压入栈。所以，栈顶元素具有最小的f（x）值。弹出栈顶元素并和目标节点比较，如果栈顶元素不是目标，则扩展栈顶元素，并计算其所有子状态的f值，并按升序把这些子状态压人栈中。如果栈顶元素是目标，则算法退出，否则该过程会循环下去，直到栈为空。

![20220322162241-2022-03-22-16-22-42](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220322162241-2022-03-22-16-22-42.png)

### 模拟退火算法

- 需要模拟退火算法在函数f产生了没有比当前状态更好的下一个状态时，指出搜索的方向。这样，对所有可能的下一个合理状态计算AE值，并用下式计算p'值：`p'= exp（-ΔΕ/T）`
  在闭区间[0，1]中随机得到一个数字，然后和p'比较。如果p'大，则选择它作为下一个转换状态。参数T在搜索程序中是逐渐降低的。这时由于T降低，p'也降低，从而使得算法终止在一个稳定的状态。

![20220322162306-2022-03-22-16-23-07](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220322162306-2022-03-22-16-23-07.png)

### 最好优先算法

- 在下面介绍的最好优先算法中，搜索是从最有希望的节点开始，并且生成其所有的子女节点。然后计算每个节点的性能（合适性），基于该性能选择最有希望的节点扩展。注意，这里是对所有的节点进行检测，然后选择最有希望的节点进行扩展，而不是仅仅从当前节点所生成的子节点中进行选择。

- 最好优先搜索算法是一个通用的算法，但是，该算法并没有显式地给出如何定义启发式函数，并且不能保证当存在从起始节点到目标节点的最短路径时，一定能够找到它。为此需要对启发式函数等进行限制。

![20220322162328-2022-03-22-16-23-29](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220322162328-2022-03-22-16-23-29.png)

### A*算法

- 在图搜索策略的基础上，下面给出A*算法。

- 定义评估函数f"如下：`f*(n) = g*(n) + h*(n)`

- 其中，g*(n)为起始节点到节点n的最短路径的代价，h*(n)是从节点n到目标节点的最短路径的代价。这样f*(n)就是从起始节点出发通过节点n到达目标节点的最佳路径的总代价估值。

- 把估价函数f(n)和f*(n)相比较，g(n)是对g*(n)的估计，h(n)是对h*(n)的估计。
  在这两个估计中，尽管g(n)容易计算，但它不一定就是从起始节点S。到节点n的真正最短路径的代价，很可能从初始节点S。到节点n的真正最短路径还没有找到，所以一般都有g(n) >= g*(n)。

- 有了g*(n)和h*(n)的定义，如果对最好优先的启发式搜索算法中的g(n)和h(n)作如下的限制：
  
  - 1）g(n)是对g*(n)的估计，且g(n)>0。
  
  - 2）h(n)是h*(n)的下界，即对任意节点n均有h(n) <= h*(n)。

- 则称这样得到的算法为A*算法。

---

## 11.图搜索的启发式函数、搜索算法。

### 启发式函数

- 通用图搜索算法
  - f(n) = g(n) + h(n)
  - 生成费用g(x)可以比较容易地得到，例如，如果节点 x 是从初始节点经过 m 步得到的，则g(x)应该和 m 成正比（或者就是m）。h(x)只是一个预测值。

### 搜索算法

- 通用图搜索算法
  
  - 在图搜索策略中，则明确保存所有的试探路径，使得任何一条路径可被候选作为进一步的扩展。在图搜索算法记录状态空间中那些被搜索过的状态，它们组成一个搜索图记为G。G由两种节点组成。
  
  - 一个节点称为open，如果该节点已经生成，而且启发式函数值h(x)已经计算出来，但是它还没有扩展。这些节点也称为未考察节点。
  
  - 一个节点称为closed，如果该节点已经扩展并生成了其子女。closed节点是已经考察过的节点。
  
  - 因此可以给出两个数据结构OPEN和CLOSED表，分别存放open节点和closed节点。
    根据前面的讨论，节点x总的费用函数f(x)是g(x)和h(x)之和。
  
  - ![20220322162357-2022-03-22-16-23-57](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220322162357-2022-03-22-16-23-57.png)

- 与或图的启发式搜索——AO*算法
  
  - 在A\*算法中用到了两张表：OPEN表和CLOSED表。AO\*算法只用一个结构G，它表达了至今已明显生成的部分搜索图。图中每一节点向下指向其直接后继节点，向上指向其直接前趋节点。同图中每一节点有关的还有h值，它估计了从该节点至一组解节点那条路径上的耗费。
  - AO\*算法还需用一个称为FUTILITY的值。若一解的估计耗费变得大于FUTILITY的解，则要放弃该搜索。FUTILITY应选得相当于一个阙值，使得大于耗费FUTILITY的任一解即使能找到也因为昂贵而无法应用。
  - ![20220322204732-2022-03-22-20-47-33](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220322204732-2022-03-22-20-47-33.png)
  - 对于AO\*算法，需要注意的是：
    - 1）在算法3.10中的2.4.5）步，需要将费用已改变的某一节点的所有祖先加人S中，然后修改祖先的费用。
    - 2）算法没有考虑子目标之间的相互依赖关系。

---

## 12.博弈树的概念、特点，极大极小过程，α-β过程。

### 博弈树的概念：

- ***书本上**：这里讲的博弈是二人博弈，二人零和、全信息、非偶然博弈，博弈双方的利益是完全对立的：*
  - *1）对垒的双方MAX和MIN轮流采取行动，博奔的结果只能有三种情况：MAX胜，MIN败；MAX败，MIN胜；和局。*
  - *2）在对垒过程中，任何一方都了解当前的格局和过去的历史。*
  - *3）任何一方在采取行动前都要根据当前的实际情况，进行得失分析，选择对自己最为有利而对对方最不利的对策，不存在“碰运气”的偶然因素，即双方都很理智地决定自己的行动。*
- **把双人博弈过程用图的形式表示出来，就可以得到一棵AND-OR树，这种AND-OR树称为博弈树。**
- 在博弈树中，那些下一步该MAX走的结点称为MAX结点，而下一步该MIN走的结点称为 MIN结点。
  
  > 关于与或图：
  > 用与或图可以方便地把问题归约为子问题替换集合。例如，假设问题A既可通过问题C~1~，与C~2~，也可通过问题C~3~、C~4~、和C~5~，或者由单独求解问题C~6~。来解决，如图3-6所示。图中各节点表示要求解的问题或子问题。
  > 
  > 问题C~1~，和C~2~构成后继问题的一个集合，问题C~3~、C~4~和C~5~，构成另一后继问题集合；而问题C~6~。则为第三个集合。对应于某个给定集合的各节点，用一个连接它们的圆弧来标记。图3-7中连接C~1~、C~2~和C~3~、C~4~、C~5~的圆弧分别叫作2连接弧和3连接弧。一般而言，如果某种弧叫**K连接弧**，则表示对问题A由某个操作算子作用后产生K个子问题。
  > ![20220322193448-2022-03-22-19-34-49](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220322193448-2022-03-22-19-34-49.png)
  > 
  > 由节点及K连接弧组成的图，称为与或图，当所有K均为1时，就变为普通的或图。
  > 
  > 可以对图3-6所示的与或图进行变换，引进某些附加节点，以便使含有一个以上后继问题的每个集合能够聚集在它们各自的父辈节点下。这样图3-6就变为图3-7所示的结构，每个节点的后继只包含一个K连接弧。弧连接的子节点叫作**与节点**，如C~1~，、C~2~及C~3~、C~4~、C~5~。K=1的连接弧连接的子节点叫作**或节点**。

### 博弈树特点主要如下：

- ⑴ 博弈的初始状态是初始结点；
- ⑵ 博弈树的**与结点**和**或结点**是**逐层交替出现**的；
- ⑶ 整个博弈过程**始终站在某一方的立场上**，所以能使自己一方获胜的终局都是本原问题，相应的结点也是可解结点，所有使对方获胜的结点都是不可解结点。

### 极大极小过程：

- 假定对奔双方分别为MAX和MIN，规定：
  - 1）有利于MAX方的态势，f(p)取正值。
  - 2）有利于MIN方的态势，f(p)取负值。
  - 3）态势均衡时，f(p)取零。
- MINMAX过程的基本思想是：
  - ⑴ 当轮到MIN走步的结点时，MAX应考虑最坏的情况（即应使f(p) 取极小值）。
  - ⑵ 当轮到MAX走步的结点时，MAX应考虑最好的情况（即 f(p)取极大值）。
  - ⑶ 评价往回倒推时，相应于两位棋手的对抗策略，交替使用⑴和⑵两种方法传递倒推值。
- 所以这种方法称为极大极小过程。 

### α-β过程

- 极大极小过程先生成一棵博弈搜索树，而且会生成规定深度内的所有结点，然后再进行估值的倒推计算，这样使得生成博弈树和估计值的倒推计算两个过程完全分离，因此搜索效率较低。
- 如果能边生成博弈树，边进行估值的计算，则可能不必生成规定深度内的所有结点，以减少搜索的次数，这就是下面要讨论的 α-β过程。
- **α-β过程把生成后继和倒推值估计结合起来，及时剪掉一些无用分支**，以此来提高算法的效率。
- 在 α-β过程中，一个结点的倒推值是动态进行计算的。
- 首先使搜索树的某一部分达到最大深度，计算出某些 MAX结点的暂时的α 值，或者是某些MIN结点的暂时的 β值；
- 随着搜索的继续，不断修正个别结点的α或β值。
- 当初始结点的倒推值确定以后，终止整个α-β过程。
- 注意， MAX结点的α值和MIN 结点的β 值的修改有如下规律：MAX结点的 α值永不下降；MIN结点的 β值永不增加。
- 剪枝的规则：
  - ⑴ 若**任何 MIN结点的β 值小于或等于任何它的先辈MAX结点（不一定是父结点）的α 值，则可终止该MIN结点以下的搜索**，然后这个MIN结点的最终倒推值即为它已得到的 β值。
  - ⑵ 若**任何 MAX结点的α值大于或等于它的 MIN先辈结点（不一定是父结点）的 β值，则可以终止该 MAX 结点以下的搜索**，然后这个MAX结点处的倒推值即为它已得到的α值。
- 当满足规则⑴而减少了搜索时，进行了α‑剪枝；而当满足规则⑵而减少了搜索时，进行了 β‑剪枝。
- 在发生了剪枝的情况下，一个 MAX结点的 α值或者一个 MIN结点的β 值未必与真正的极大极小值的搜索结果的倒推值相同
- 初始结点的倒推值必然是相同的，使用它选择的走步也是相同的。
- **动态地计算并保存α 和β 值，并且一旦可能就进行剪枝的过程称为α-β 过程**，当初始结点的全体后继结点的最终倒推值全部给出时，上述过程便结束。
- 在搜索深度相同的条件下，采用这个过程所获得的走步总跟简单的极大极小过程的结果是相同的，区别只在于α-β 过程通常只用少得多的搜索便可以找到一个理想的走步。

---

## 13.归结演绎推理的概念、过程、算法、归结反演及搜索策略。

### 归结演绎推理的概念

- ***书本上**：归结演绎推理本质上就是一种反证法，它是在归结推理规则的基础上实现的。为了证明一个命题P恒真，它证明其反命题﹁P恒假，即不存在使得﹁P为真的解释。*
- 来自网络：归结演绎推理是一种基于逻辑“反证法”的机械化定理证明方法。其基本思想是把永真性的证明转化为不可满足性的证明。即要证明 P → Q（﹁ P ∨ Q）永真，只要能够证明 P ∧ ﹁ Q 为不可满足即可。(FROM.[人工智能 —— 归结演绎推理](https://blog.csdn.net/starter_____/article/details/88797385))

### 归结演绎推理的过程

- 来自网络：谓词公式不可满足的充要条件是其子句集不可满足。因此，要**把谓词公式转换为子句集**，再**用鲁滨逊归结原理求解子句集是否不可满足**。如果子句集不可满足，则 P → Q 永真。

### 归结演绎推理的算法

- 置换和合一
  - 置换
    - ![20220322223756-2022-03-22-22-38-00](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220322223756-2022-03-22-22-38-00.png)
    - ![20220322223833-2022-03-22-22-38-35](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220322223833-2022-03-22-22-38-35.png)
    - 性质：
      - ![20220322224842-2022-03-22-22-48-45](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220322224842-2022-03-22-22-48-45.png)
      - ![20220322224905-2022-03-22-22-49-07](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220322224905-2022-03-22-22-49-07.png)
    - ![20220322224928-2022-03-22-22-49-31](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220322224928-2022-03-22-22-49-31.png)
    - ![20220322225114-2022-03-22-22-51-17](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220322225114-2022-03-22-22-51-17.png)
  - 合一
    - 本节将对有限非空可合一的表达式集合给出求取最一般合一置换的合一算法。当集合不可合一时，算法也能给出不可合一的结论，并且结束。
    - **定义3.10** 表达式的非空集合W的差异集是按下述方法得出的子表达式的集合：
      - 1）在W的所有表达式中找出对应符号不全相同的第一个符号（自左算起）。
      - 2）在W的每个表达式中，提取出占有该符号位置的子表达式。这些子表达式的集合便是W的差异集D。
    - ![20220322230928-2022-03-22-23-09-31](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220322230928-2022-03-22-23-09-31.png)
    - **定理3.2** 若W为有限非空可合一表达式集合，则合一算法总能终止在第2）步上，并且最后的o便是W的**最一般合一（MGU）**。
- 归结式
  - ![20220322231428-2022-03-22-23-14-34](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220322231428-2022-03-22-23-14-34.png)
  - ![20220322231450-2022-03-22-23-14-54](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220322231450-2022-03-22-23-14-54.png)
  - ![20220323115937-2022-03-23-11-59-38](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220323115937-2022-03-23-11-59-38.png)

### 归结反演

谓词逻辑的归结反演是仅有一条推理规则的问题求解方法，为证明A → B，其中A，B是谓词公式。使用反演过程，先建立合式公式：G = A ∧ B进而得到相应的子句集S，只需证明S是不可满足的即可。

归结反演不仅可以用于定理证明，而且可以用来求取问题的答案，其思想与定理证明类似。方法是在目标公式的否定形式中加上该公式否定的否定，得到重言式；或者再定义一个新的谓词ANS，加到目标公式的否定中，把新形成的子句加到子句集中进行归结。

### 归结反演搜素策略

- 排序策略
  - 宽度优先
  - 深度优先
  - 单元优先策略：优先考虑仅由一个文字构成的字句（单元子句）。
- 精确策略
  - 支持集策略
    - 支持集策略就是指，每次归结时，参与归结的子句中至少应有一个是由目标公式的否定所得到的子句，或者是它们的后裔。
    - 所谓后裔是指，如果（I）a~2~是a~1~与另外某子句的归结式，或者（I）a~2~是a~1~的后裔与其他子句的归结式，则称a~2~是a~1~的后裔，a~1~是a~2~的祖先。
    - 支持集策略是完备的，即假如对一个不可满足的子句集合运用支持集策略进行归结，那么最终会导出空子句。
  - 线性输入策略
    - 线性输入策略是指，参与归结的两个子句中至少有一个是原始子句集中的子句（包括那些待证明的合式公式的否定）。
    - 线性输入策略是不完备的。
  - 祖先过滤策略
    - 由于线性输人策略是不完备的，改进该策略则得到祖先过滤策略：参与归结的两个子句中至少有一个是初始子句集中的句子，或者是另一个子句的祖先。
    - 该策略是完备的。

---

## 14.产生式系统的推理、自然演绎推理、非单调推理。

### 产生式系统的推理

- 产生式系统：综合数据库+产生式规则库+控制系统
- 正向推理（数据驱动推理）
  - ![20220323154842-2022-03-23-15-48-43](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220323154842-2022-03-23-15-48-43.png)
- 反向推理
  - ![20220323154856-2022-03-23-15-48-56](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220323154856-2022-03-23-15-48-56.png)
- 混合推理
  - 适用情况：
    - 将正向推理和反向推理结合起来，取长补短。
    - 已知的事实不充分
    - 由正向推理推出的结论可信度不高
    - 希望得到更多结论
  - 混合推理分为两种情况：
    - 一种情况是先进行正向推理，帮助选择某个目标，即从已知事实演绎出部分结果，然后再用反向推理证实该目标或提高其可信度；
    - 另一种情况是先假设一个目标进行反向推理，然后再利用反向推理中得到的信息进行正向推理，以推出更多的结论。

### 自然演绎推理

自然演绎推理是从一组已知为真的事实出发，直接运用经典逻辑的推理规则，推出结论的过程。其中，基本的推理规则有**P规则**、**T规则**、**假言推理**和**拒取式推理**等。P规则是指在推理的任何步骤上都可以引入前提，继续进行推理。T规则是指在推理时，如果前面步骤中有一个或多个公式永真蕴含S，则可以把S引人到推理过程中。

### 非单调推理

在日常生活中，人们通常并不是在掌握了所有证据的完全信息之后才开始解决问题的，而是可以**在有些信息未知的情况下**，作出一些**合理的假设**，这常常需要进行默认或限制等形式的推理，非单调推理就是针对这类问题而提出的。

- 默认推理
  - 在非单调推理中，默认逻辑是一种应用比较广泛的逻辑，它是在信息不完全和前提默认的情况下，默认一些先决条件而进行的推理。
  - 传统的逻辑是从已知的事实推出新的事实，在推理时，知识库的丰富程度决定了能推出多少事实[Reiter 1980]。
  - 而在非单调推理中，知识库不够丰富，难以支持系统所需要的推理，因此需要对知识库进行扩充，这些扩充的知识就是默认的知识。
  - 默认知识并不十分可靠，只是在目前看来不和知识库的其他部分发生矛盾，所以推出来的不能算是事实，只是对现实世界的一种猜测。
  - ![20220323160058-2022-03-23-16-00-59](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220323160058-2022-03-23-16-00-59.png)
- 限制推理
  - 如果一个句子叙述一个命题，那么它叙述的仅仅是这个命题，一点都不能扩张和延伸，任何多余的东西都要删除掉。这就是所谓的“Occam剃刀原理”，麦卡锡称之为极小模型。它试图最小化谓词的解释，以减少非单调的影响范围。
  - 在早期，限制理论的概念是论域限制，它的含义是：只有那些存在的事物才是已知的，未知的事物都是不知道的。
  - ![20220323160243-2022-03-23-16-02-43](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220323160243-2022-03-23-16-02-43.png)

---

## 15.不确定性知识分类、不确定性推理的基本问题、推理方法分类。

### 不确定性知识分类

根据不确定性产生的原因及表现形式，可以将不确定性大致分为以下5种类型。

- 随机不确定性
  - ①事件可以在基本相同的条件下重复进行
  - ②在基本相同条件下某事件可能以多种方式表现出来，事先不能确定它以何种特定方式发生
  - ③事先可以预见该事件以各种方式出现的所有可能性，预见它以某种特定方式出现的概率，即在重复过程中出现的频率
- 模糊不确定性
- 不完备性
  - 知识的不完备性包括**知识内容的不完整**、**知识结构的不完备**等。
  - 内容的不完整，可能来源于获取知识时观测不充分、设备不精确，只获取了局部信息，因此对部分信息内容根本不知道，或者知道应该有某一个具体的信息值，但不清楚其大小。
  - 知识结构的不完备，可能因为人的认识能力、获取手段的限制等，造成对解决某个特定问题的背景和结构认识不全，忽略了一些重要因素。
- 不协调性
  - 知识的不协调是指**知识内在的矛盾**，不协调的程度可以依次为**冗余**、**干扰**和**冲突**等。
  - 不协调性是知识不确定性的重要体现，人们不可能、也没必要在一切场合下都试图消除知识的不协调性，固执地去追求知识的一致性，要把不协调看作是知识的一种常态，允许包容、并蓄，允许折中、调和。
- 非恒常性
  - 非恒常性主要是指知识随时间的变化而变化。

### 不确定性推理的基本问题

- **表示问题**
  
  - 描述不确定性方法：
    - 数值表示：便于计算、比较
    - 非数值表示：定性的描述，以便较好地解决不确定性问题
  - 专家系统中不确定性分类：
    - 知识的不确定性：知识或规则强度
    - 证据的不确定性：动态强度

- **计算问题**
  
  - 计算问题主要指不确定性的传播与更新，也即获得新信息的过程。
  - 它是在领域专家给出的规则强度和用户给出的原始证据的不确定性的基础上，定义一组函数，求出结论的不确定性度量。它主要包括以下3个方面：
    - （1）**不确定性的传递算法**
      - 1）在每一步推理中，如何把证据及知识的不确定性传递给结论。
      - 2）在多步推理中，如何把初始证据的不确定性传递给结论。
    - （2）**结论不确定性合成**
      - 推理中有时会出现这样的一种情况，用不同的知识进行推理，得到了相同结论，但不确定性的程度却不相同。
    - （3）**组合证据的不确定性算法**
    - 已知证据E~1~和E~2~的不确定性度量C(E~1~)和C(E~2~)，求证据E~1和E~2~的析取和合取的不确定性。
      - 最大最小法
        - ![20220323211352-2022-03-23-21-13-54](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220323211352-2022-03-23-21-13-54.png)
      - 概率方法
        - ![20220323211409-2022-03-23-21-14-11](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220323211409-2022-03-23-21-14-11.png)
      - 有界方法
        - ![20220323211422-2022-03-23-21-14-25](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220323211422-2022-03-23-21-14-25.png)

- **语义问题**
  
  - 语义问题指上述表示和计算的含义是什么？
  
  - 处理不确定性问题的主要数学工具有**概率论**和**模糊数学**。
    
    - **概率论**研究和处理随机现象，事件本身有明确的含义，只是由于条件不充分，使得在条件和事件之间不能出现决定性的因果关系（随机性）。
    - **模糊数学**研究和处理模糊现象，概念本身就没有明确的外延，一个对象是否符合这个概念是难以确定的（属于模糊的）。
  
  - 度量知识和证据的不确定性
    
    - 规则的不确定性度量f(H，E)，需要定义在下述3个典型情况下的取值：
      
      > 若E为真，则H为真，这时f(H，E)=?
      > 
      > 若E为真，则H为假，这时f(H，E)=?
      > 
      > E对H没有影响，这时f(H，E)=?
    
    - 对于证据的不确定性度量C(E)，需要定义在下述3个典型情况下的取值：
      
      > E为真，C(E)=?
      > 
      > E为假，C(E)=?
      > 
      > 对E一无所知，C(E)=? 

### 推理方法分类

- 控制方法
  - 在控制策略级处理不确定性的方法，其特点是通过识别领域中引起不确定性的某些特征及相应的控制策略来限制或减少不确定性对系统产生的影响。
  - 这类方法没有处理不确定性的统一模型，其效果极大地依赖于控制策略，我们把这类方法统称为控制方法。
  - 目前常用的控制方法有**启发式搜索**、**相关性制导回潮**和**机缘控制**等。
- 模型方法
  - 在推理一级上扩展不确定性推理的方法，其特点是把不确定证据和不确定的知识分别与某种量度标准对应起来，并且给出更新结论不确定性算法，从而建立不确定性推理模式。
  - 模型方法分类：
    - 数值方法：对不确定性的一种定量表示和处理方法，是本章研究的重点。
      - 概率方法：发展出了新的方法
        - **可信度方法**
        - **证据理论**
        - 主观概率论（**主观贝叶斯方法**）
    - 非数值方法：指除数值方法外的其他各种处理不确定性的方法。
      - 古典逻辑
      - 非单调推理方法

---

## 16.主观Bayes方法、可信度方法和证据理论方法知识表示形式、不确定性复合、传播和积累计算。

### 主观Bayes方法

- 贝叶斯公式
  
  - 主观Bayes方法**在概率论的基础**上，通过**对Bayes公式的修正**而形成的一种不确定性推理模型
  
  - 基础公式
      ![20220323221919-2022-03-23-22-19-20](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220323221919-2022-03-23-22-19-20.png)
      
  - 公式计算：
    - ![20220323222841-2022-03-23-22-28-41](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220323222841-2022-03-23-22-28-41.png)
    - ![20220323222858-2022-03-23-22-28-58](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220323222858-2022-03-23-22-28-58.png)
    - 贝叶斯推理的优点是具有较强的理论背景和良好的数字特性，当证据和结论都彼此独立时，计算的复杂度比较低。但是它也有其局限性：
      - 1）因为需要ΣP(H~j~)= 1，如果又增加一个新的假设，则对所有的1 <= j <= n+1，j=1，P(H~j~)都需要重新定义。
      - 2）贝叶斯公式的应用条件是很严格的，它要求各事件互相独立，若证据间存在依赖关系，就不能直接使用此方法。
      - 3）在概率论中，一个事件或命题的概率是在大量统计数据的基础上计算出来的，因此尽管有时P(E~j~|H~i~)比P(H~i~|E~j~)相对容易得到，但总的来说，要想得到这些数据仍然是一件相当困难的工作。

- 知识不确定性
  
  - 信任几率
    - 如果事件或命题不可重复，则在一般意义，上条件概率P（A|B）是一个不必要的概率。这时可以把P（A|B）解释为在B成立时A为真的可信度（Degree of Belief）。
    - 如果P（A|B）=1，则可以相信A为真的；如果P（A|B）=0，则可以相信A是假的。而对于其他值0 < P（A|B）< 1，则表示不能完全确定A是真是假。
    - 在统计学上，一般认为假设就是依据某些证据还不能确定其真假的命题，这样可以使用条件概率来表示似然性（Likelihood）。
    - 概率适用于重复事件，而似然性适用于表示非重复事件中信任的程度。
    - 一般在专家系统，中，P（H|E）表示在有证据E的情况下，专家对某种假设H为真的信任度。表达这种似然性的方法可以采用赌博中的几率（Odds）方法。
    - ![20220324155312-2022-03-24-15-53-14](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324155312-2022-03-24-15-53-14.png)
    - P(x)=0时，有0(x)=0；P(x)=1时，有0(x)= ထ
    - 这样，就可以把取值为 [ 0，1 ] 的P(x)放大到取值为 [ 0，+ထ )的0(x)。
  - 充分性和必然性
    - 定义似然率（Likelihood Ratio）LS如下：
      - ![20220324155945-2022-03-24-15-59-46](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324155945-2022-03-24-15-59-46.png)
      - 贝叶斯定理的**几率似然性形式**如下，其中因子LS称为充分似然性
        - `O(H|E) = LS · O(H)`——————（4.33）
      - LN公式：
        - ![20220324160151-2022-03-24-16-01-53](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324160151-2022-03-24-16-01-53.png)
      - 贝叶斯定理的**必然似然性形式**如下
        - `O(H|﹁E) = LN · O(H)`——————（4.36）
      - 式（4.33）和（4.36）就是修改的贝叶斯公式。从这两个公式可以看出：**当E为真时**，可以利用LS将H的先验几率0（H）更新为其后验几率0（H|E）；**当E为假时**，可以利用LN将H的先验几率0（H）更新为其后验几率0（H|﹁E）。
      - LN：LS反映的是E的出现对H为真的影响程度。因此，称LS为**知识的充分性度量**。
        - 当LS>1时，O（H|E）> O（H），说明E支持H；LS越大，O（H|E）比O（H）大得越多，即LS越大，E对H的支持越充分。当LS→ထ时，O（H|E）→ထ，即P（H|E）→1，表示由于E的存在，将导致H为真。
        - 当LS=1时，O（HE）=O（H），说明E对H没有影响。
        - 当LS<1时，O（H|E）< O（H），说明E不支持H。
        - 当LS=0时，O（H|E）= 0，说明E的存在使H为假。
      - LS：LN反映的是当E不存在时对H为真的影响。因此，称LN为**知识的必要性度量**。
        - 当LN>1时，O（H|﹁E）> O（H），说明﹁E支持H，即由于E的不出现，增大了H为真的概率。并且，LN越大，P（H|﹁E）就越大，即﹁E对H为真的支持就越强。当LN→ထ时，O（H|﹁E）→ထ，即P（H|﹁E）→1，表示由于﹁E的存在，将导致H为真。
        - 当LN=1时，O（H|﹁E）= O（H），说明﹁E对H没有影响。
        - 当LN<1时，O（H|﹁E）< O（H），说明﹁E不支持H，即由于﹁E的存在，将使H为真的可能性下降，或者说由于﹁E不存在，将反对H为真。当LN>0时，O（H|﹁E）→0，即LN越小，E的不出现就越反对H为真，这说明H越需要E的出现。
        - 当LN=0时，O（H|﹁E）= O，说明﹁E的存在（即E不存在）将导致H为假。
      - LS与LN的关系
        - 由于E和-E不会同时支持或同时排斥H，因此只有下述3种情况存在：
          - 1）LS>1 ELN<1。
          - 2）LS>1且LN>1。
          - 3）LS = LN =1
  - 规则表示方式
    - 在主观贝叶斯方法中，规则是用产生式表示的，其形式为`IF E THEN（LS，LN）H` 其中，（LS，LN）用来表示该规则的强度。
    - 在实际系统中，LS和LN的值均是由领域专家根据经验给出的，而不是由LS和LN计算出来的。当证据E越是支持H为真时，则LS的值应该越大；当证据E对H越是必要时，则相应的LN值应该越小。

- 证据不确定性
  
  - 在主观贝叶斯方法中，证据E的不确定性可以用证据的似然性或几率来表示。似然率，与几率之间的关系为
  - ![20220324163459-2022-03-24-16-35-01](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324163459-2022-03-24-16-35-01.png)

- 组合证据不确定性
  
  - 多个单一证据的合取（AND）
    - P(E|S) = min{P(E~1~|S) , P(E~2~|S) , ... , P(E~n~|S)}
  - 多个单一证据的析取（OR）
    - P(E|S) = max{P(E~1~|S) , P(E~2~|S) , ... , P(E~n~|S)}
  - 非运算
    - P(﹁E|S) = 1 - P(E|S)

- 不确定性的传递
  
  - 主观贝叶斯方法推理的任务就是根据E的概率P（E）及LS、LN的值，把H的先验概率（或似然性）P（H）或先验几率O（H）更新为后验概率（或似然性）或后验几率。
  - 证据肯定为真
    - 先验几率更新为后验几率：`O(H|E) = LS · O(H)`
    - 先验概率更新为后验概率：![20220324172303-2022-03-24-17-23-05](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324172303-2022-03-24-17-23-05.png)
  - 证据肯定为假
    - 先验几率更新为后验几率：`O(H|﹁E) = LN · O(H)`
    - 先验概率更新为后验概率：![20220324172402-2022-03-24-17-24-03](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324172402-2022-03-24-17-24-03.png)
  - 证据既非真又非假
    - `P(H|S) = P(H|E) P(E|S) + P(H|﹁E) P(﹁E|S)`
    - P(E|S) = 1（此时证据肯定存在，P(﹁E|S) = 0）
      - 则![20220325171419-2022-03-25-17-14-20](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220325171419-2022-03-25-17-14-20.png)
    - P(E|S) = 0（此时证据肯定不存在，P(﹁E|S) = 1）
      - 则![20220324172850-2022-03-24-17-28-52](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324172850-2022-03-24-17-28-52.png)
    - P(E|S) = P(E)（此时E与S无关）
      - 则![20220324173022-2022-03-24-17-30-23](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324173022-2022-03-24-17-30-23.png)
    - R(E|S)为其他值
      - 则![20220324173059-2022-03-24-17-31-01](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324173059-2022-03-24-17-31-01.png)

- 结论不确定性的合成
  
  - ![20220324173606-2022-03-24-17-36-08](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324173606-2022-03-24-17-36-08.png) 

### 可信度方法

这种模型和基于重复事件出现频率有关的普通概率不同，它基于利用某些证据来证实假设的方法，因此称为基于认知概率或确认度的确定性理论。

- 可信度模型
  
  该方法采用可信度CF（Certainty Factor）作为不确定性的测度，通过对CF(H,E)的计算，探讨证据E对假设H的定量支持程度
  
  - 可信度
    - CF(H,E) = MB(H,E) - MD(H,E)
    - CF：由证据E得到假设H的可信度
    - MB：信任增长长度
      - ![20220324174429-2022-03-24-17-44-32](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324174429-2022-03-24-17-44-32.png)
    - MD：不信任增长长度
      - ![20220324174440-2022-03-24-17-44-42](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324174440-2022-03-24-17-44-42.png)
    - P(H)：H的先验概率
    - P(H|E)：在前提条件E所对应的证据出现的情况下，结论H的条件概率。
    - 得到CF计算公式
      - ![20220324174725-2022-03-24-17-47-26](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324174725-2022-03-24-17-47-26.png)
    - CF，MB，MD的性质：
      - 互斥性
        - 对于同一证据，它不可能既增加对H的信任程度，又同时增加对H的不信任程度，这说明MB与MD是互斥的。
        - 当MB(H,E) > 0 时，MD(H,E) = 0
        - 当MD(H,E) > 0 时，MB(H,E) = 0
      - 值域
        - MB(H,E)——[ 0 , 1 ]
        - MD(H,E)——[ 0 , 1 ]
        - CF(H,E)——[ -1 , 1 ]
      - 典型值
        - CF(H,E) = 1
          - 有P(H,E) = 1
          - 说明由于E所对应证据的出现使H为真
          - 此时，MB(H,E) = 1，MD(H,E) = 0。
        - CF(H,E) = -1
          - 有P(H,E) = 0
          - 说明由于E所对应证据的出现使H为假
          - 此时，MB(H,E) = 0，MD(H,E) = 1。
        - CF(H,E) = 0
          - 有P(H,E) = P(H)
          - 表示H与E独立，即E所对应的证据的出现对H没有影响
      - 对H的信任增长度等于对非H的不信任增长度
        - MD(﹁H,E) = MB(H,E)
        - CF(H,E) + CF(﹁H,E) = 0
        - ①对H的信任增长度等于对非H的不信任增长度
        - ②对H的可信度与对非H的可信度之和等于0
        - ③可信度不是概率。
      - 5）对同一前提E，若支持若干个不同的结论H（i=1, 2, …, n），则
        `Σ CF(H~1~,E) <= 1`
  - 可信度计算
    - 规则不确定性表示`IF E THEN N (CF(H,E))`
    - 证据不确定性表示
      - 当证据E肯定为真时，CF(E) = 1
      - 当证据E肯定为假时，CF(E) = -1
      - 当证据E一无所知时，CF(E) = 0
    - 组合证据不确定性的计算
      - 多个单一证据的合取（AND）
        - CF(E) = min{CF(E~1~) , CF(E~2~) , ... , CF(E~n)}
      - 多个单一证据的析取（OR）
        - CF(E) = max{CF(E~1~) , CF(E~2~) , ... , CF(E~n)}
      - 非运算
        - CF(﹁E) = ﹁CF(E)
    - 不确定性推理
      - 证据肯定存在（CF(E) = 1）时
        - CF(H) = CF(H,E)
      - 证据不是肯定存在（CF(E) != 1）时
        - CF(H) = CF(H,E) · max{0 , CF(E)}
      - 证据是多个条件组合的情况。即如果有两条规则推出一个相同结论，并且这两条规则的前提相互独立，结论的可信度又不相同，则可用不确定性的合成算法求出该结论的综合可信度。
        - ![20220324200712-2022-03-24-20-07-17](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324200712-2022-03-24-20-07-17.png)

- 说明
  
  - 可信度计算
    - MYCIN中可信度修改为：
      - ![20220324201312-2022-03-24-20-13-19](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324201312-2022-03-24-20-13-19.png)
    - 可信度方法特点
      - 1）CF值可能与条件概率得出的值相反
      - 2）通常`P(H|E) != P(H|S) P(S|E)`其中S是基于证据E的某些中间假设。但在推理链中的两条规则的确定因子确是作为独立概率计算的：
        `CF(H,E) != CF(H,S) CF(S,E)`
      - 3）MYCIN一般应用于推理链短，且假设简单的问题中，如果把该方法应用于不具备短推理链、简单假设的领域，则可能会出问题。
      - 4）由于可能导致计算的累计误差，如果多个规则逻辑等价于一个规则，则采用一个规则和多个规则计算的CF值可能就不相同。
      - 5）组合规则使用的顺序不同，也可能得到不同的结果。

### 证据理论方法

  证据理论（Theory of Evidence）称为D-S（Dempster-Shafer）理论。

- 假设的不确定性
  
    在D-S理论中，可以分别用**信任函数**、**似然函数**和**类概率函数**来描述知识的**精确信任度**、**不可驳斥信任度**及**估计信任度**，即可以从各个不同角度刻画命题的不确定性。
  
    D-S理论采用集合来表示命题，为此，首先应该建立命题与集合之间的一一对应关系，把命题的不确定性问题转化为集合的不确定性问题。
  
    设Q为变量x的所有可能取值的有限集合（亦称为样本空间），且Ω中的每个元素都相互独立，则由Ω的所有子集构成的幕集记为2^Ω^。当Q中的元素个数为N时，则其幕集2^Ω^的元素个数为2^N^，且其中的每一个元素A都对应于一个关于x的命题，称该命题为“x的值在A中”。
  
  - 概率分配函数
    - ![20220324203155-2022-03-24-20-31-59](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324203155-2022-03-24-20-31-59.png)
    - m(A)表示对A的精确信任度，  
    - m是2^Ω^上而非Ω上的概率分布，所以基本概率分布函数不是概率
  - 信任函数
    - ![20220324203608-2022-03-24-20-36-14](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324203608-2022-03-24-20-36-14.png)
    - 当A为单一元素组成的集合时，Bel(A)=m(A)。
    - 如果命题“x在B中”成立，必带有命题“x在A中”成立。
    - Bel(A)函数又称为下限函数。
  - 似然函数
    - ![20220324203846-2022-03-24-20-38-50](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324203846-2022-03-24-20-38-50.png)
    - ![20220324204125-2022-03-24-20-41-31](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324204125-2022-03-24-20-41-31.png)
    - 信任函数和似然函数有如下性质：
      - ![20220324204207-2022-03-24-20-42-13](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324204207-2022-03-24-20-42-13.png)
    - 由于Bel(A)和Pl(A)分别表示A为真的信任度和A为非假的信任度，因此，可分别称Bel(A)和Pl(A)为对A信任程度的下限和上限，记为`A(Bel(A),PI(A))`,Pl(A)-Bel(A)表示既不信任A，也不信任-A的程度，即表示对于A是真是假不知道的程度。
  - 类概率函数f(A)
    - ![20220324204558-2022-03-24-20-46-04](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324204558-2022-03-24-20-46-04.png)
    - 性质
      - ![20220324204626-2022-03-24-20-46-32](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324204626-2022-03-24-20-46-32.png)

- 证据的组合函数
  
  - ![20220324204857-2022-03-24-20-49-02](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324204857-2022-03-24-20-49-02.png)

- 规则的不确定性
  
  - 具有不确定性的推理规则可表示为`IF E THEN H , CF`其中，H为假设，E为支持H成立的假设集，它们是命题的逻辑组合；CF为可信度因子。
  - H可表示为H={a~1~, a~2~, ..., a~m~}a~i~∈Ω(i=1, ..., m)，H为假设集合Q的子集。
  - CF ={c~1~, c~2~, ..., c~m~}，c~i~用来描述前提E成立时a~i的可信度。- CF应满足如下条件：
    - 1）c~i~ >= 0，1 <= i <= m
    - 2）Σ c~i~ <= 1

- 不确定性的组合
  
  - 合取或析取
    - f(E~1~ΛE~2~Λ...ΛE~n~) = min {f(E~1), f(E~2), ..., f(E~n)}
    - f(E~1~VE~2~V...VE~n~) = max {f(E~1), f(E~2), ..., f(E~n)}
  - ![20220324214800-2022-03-24-21-48-02](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324214800-2022-03-24-21-48-02.png)

---

## 17.模糊逻辑与模糊推理。

### 模糊集合及其运算

- 为表示一些模糊概念，扎德于1965年提出模糊集合理论，其基本思想就是把传统集合论中由特征函数决定的绝对隶属关系模糊化，把集合 ( 0，1 ) 扩散到区间 [ 0，1 ] ，使元素x对子集A的隶属程度不再局限于取0或1，而是可以取集合 [ 0，1 ] 上的任何值，以表示元素x隶属于子集A的模糊程度。
- ![20220324215647-2022-03-24-21-56-49](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324215647-2022-03-24-21-56-49.png)
- ![20220324220015-2022-03-24-22-00-16](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324220015-2022-03-24-22-00-16.png)
- 一个由单个成员构成的模糊集合称为**单点集**（Singleton）。单点集所处的点称为**支撑点**（Support Point），或**单点集的支撑值**（Support Value）。一个单点集的隶属函数，在支撑点以外的变量空间中的任何地方均取值为0，在支撑点处，其隶属度取值为1。
- 并、交、余运算：
  - ![20220324220247-2022-03-24-22-02-49](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324220247-2022-03-24-22-02-49.png)

### 语言变量

- 语言变量可以看作是用某种自然语言和人工语言的词语或句子来表示变量的值和描述变量间的内在联系的一种系统化的方法，它为近似推理中变量值的表示和模糊命题的真值、概率值和可能值的表示提供了一种基本的方法。
- 模糊集合和语言变量可用于量化自然语言的含义，因而可用来处理具有指定值的语言变量。语言变量取值范围是一个项目集，该集合中的元素一般可以分为基本语言项和含修饰词的语言项。
- 语言变量常常可以用于启发式规则中
- 某些语言变量可以是二阶模糊集合。

### 模糊逻辑

- 用来处理不确定性，以及模拟常识推理。
- 模糊逻辑的基本思想是将常规数值变量模糊化，使变量成为以**定性术语**（也称语言值）为值域的语言变量。模糊逻辑的核心概念是语言变量，当用语言变量来描述对象时，这些定性术语就构成模糊命题。**如果省略被描述的对象**，则模糊命题可表示为“（语言变量）（定性值）”形式。
- 每个模糊命题均由相应的一个模糊集作细化描述，所以模糊逻辑操作与模糊集操作是一致的。
- 模糊逻辑运算符：
  - ![20220324220903-2022-03-24-22-09-05](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220324220903-2022-03-24-22-09-05.png)

### 模糊推理

- 模糊推理有多种模式，其中最重要的且广泛应用的是基于**模糊规则**的推理。模糊规则的**前提是模糊命题的逻辑组合**（经由合取、析取和取反操作）作为推理的条件；**结论是表示推理结果的模糊命题**。所有模糊命题成立的精确程度（或模糊程度）均以相应语言变量定性值的隶属函数来表示。
- 一个模糊推理规则是一条表示变量间依赖关系的语句，格式：
  `IF < condition > THEN < consequence >`

---

## 18.机器学习的概念、模型和算法。

### 机器学习的概念

- 基于历史经验的，描述和预测的理论、方法和算法。
- 从人工智能的角度看，机器学习是一门研究使用计算机获取新的知识和技能，提高现有计算机求解问题能力的科学
- ***书本上**：机器学习是研究机器模拟人类的学习活动，获取知识和技能的理论和方法，以改善系统性能的学科。*

### 机器学习的模型

- ![20220325134428-2022-03-25-13-44-31](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220325134428-2022-03-25-13-44-31.png)
- **环境**：外部信息的来源，它将为系统的学习提供有关信息
- **知识库**：代表系统已经具有的知识
- **学习环节**：系统的学习机构，它通过对环境的感知取得外部信息，然后经分析、综合、类比、归纳等思维过程获得知识，生成新的知识或改进知识库的组织结构。
- **执行环节**：基于学习后得到的新的知识库，执行一系列任务，并将运行结果报告学习环节，以完成对新知识库的评价，指导进一步的学习工作,是该模型的核心。 

### 机器学习的算法

- 归纳学习
  - AQ算法
  - 变形空间算法
  - ID3算法
- 类比学习
- 分析学习
- 发现学习
- 遗传学习
- 连接学习

---

## 19.决策树概念、构造算法、ID3算法。

### 基础

- 归纳学习由于依赖于经验数据，因此又称为经验学习（Empirical Learning），由于归纳依赖于数据间的相似性，所以也称为基于相似性的学习（Similarity Based Learning）

### 决策树的概念

- 在CLS（概念学习系统）的决策树中，节点对应于**待分类对象的属性**，由某一节点引出的弧对应于**这一属性可能取的值**，叶节点对应于**分类的结果**。

### 决策树的构造算法

- 训练集：TR
- 分类对象的属性表：AttrList——[ A~1~, A~2~, ..., A~n~ ] (n >= 1)
- 全部分类结果集合：Class——[ C~1~, C~2~, ..., C~m~ ] (m >= 2)
- 每个属性A~i~的值域：ValueType(A~i~)
- TR的一个元素表示为：< X, C >
  - X=(a~1~, a~2~, ..., a~n~)
    - a~i~对应于实例第i个属性的取值
  - C：属于CLass，是实例X的分类结果
- ![20220325145456-2022-03-25-14-54-58](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220325145456-2022-03-25-14-54-58.png)

### ID3算法

- 基本的决策树学习算法ID3通过自顶向下构造决策树来进行学习。
- 构造过程是从“哪一个属性将在树的根节点被测试？”这个问题开始的。为了回答这个问题，使用统计测试来确定每一个实例属性**单独分类训练样例的能力**。分类能力最好的属性被选作树的根节点的测试。
- 然后为根节点属性的每个可能值产生一个分支，并把训练样例排列到适当的分支（也就是，样例的该属性值对应的分支）之下。然后重复整个过程，用每个分支节点关联的训练样例来选取在该点被测试的最佳属性。
- 这形成了对合格决策树的贪婪搜索，也就是算法从不回潮重新考虑以前的选择。
- ![20220325144232-2022-03-25-14-42-34](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220325144232-2022-03-25-14-42-34.png)
- 昆兰的扩展属性选取方法
  1. 子集S的熵
     - 给定正负实例的子集为S，构成训练窗口。用P~i~表示第i类输出所占训练窗口中总的输出数量的比例,当决策有k个不同的输出时，则S的熵为
     - ![20220325144815-2022-03-25-14-48-17](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220325144815-2022-03-25-14-48-17.png)
     - Pos和Neg分别表示S中正、负实例的比例，并且定义0log~2~(0)=0。对于布尔型分类：
     - ![20220325145010-2022-03-25-14-50-14](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220325145010-2022-03-25-14-50-14.png)
  2. 属性A的信息增益Gain
     - S~i~表示S中属性A的值为v~i~的子集，|S~i~|表示集合的势
     - ![20220325145242-2022-03-25-14-52-44](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220325145242-2022-03-25-14-52-44.png)

---

## 20.类比学习基本概念和方法，基于案例的推理。

### 类比学习的概念

- 类比学习是**根据两个对象之间在某些方面的相同或相似，从而推出它们在其他方面也可能相同或相似**。
- 在类比学习中，把当前所面临的对象或情形称为目标对象（Target Object），而把记忆的对象或情形称为源对象（Base Object）。
- 类比学习过程步骤
  - 联想搜索匹配
  - 检验相似程度
  - 修正变换求解
  - 更新知识库
- 相似度通过距离来定义，常用的典型距离有：
  - ![20220325163401-2022-03-25-16-34-02](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220325163401-2022-03-25-16-34-02.png)
- 事实上各属性对一个案例整体上的相似度有不同的贡献，因而还需加上权值。

### 类比学习的方法

- 类比学习的一般问题求解程序是基于**手段-目的分析**的**MEA方法**（Means-Ends Analysis）。
- 问题求解模型由两部分组成：问题空间和在此空间进行的问题求解动作[Carbonell 1986]。问题空间包括：
  - 1）一组可能的问题组合状态集。
  - 2）一个初始状态。
  - 3）两个或多个目标状态（即终止状态，为简便，假设只有一个目标状态）。
  - 4）当满足预置条件时，一组可将一个状态变为另一个状态的变换规则集（或操作符）。
  - 5）计算两个状态之间差别的函数，通常应用于比较当前状态和目标状态，得出两者间的差别。
  - 6）一个对可用的变换规则进行编序，以最大限度减少差别的索引函数。
  - 7）一组全局路径限制。其目标路径必须满足的条件，使解有效。路径限制本质上是以部分解序列为基础，而不是以单个状态或者操作符为基础。
  - 8）一个用于指示何时可用何种变换规则的差别表。
- 在问题空间上使用S-MEA算法进行问题求解操作，也就是通常所说的状态空间转换方法，找到一条从初始状态到目标状态的转换路径。
- **算法5.4** S-MEA算法
  - 1）比较当前状态和目标状态，得出差别。
  - 2）选择合适的规则（或算子），以减少两个状态间的差别。
  - 3）尽可能应用转换规则，直至完成状态转换。否则保存当前状态，并将S-MEA算法递归地用于其他子问题，直到该子问题确认不能满足该规则的前提条件为止。
  - 4）当子问题求解后，恢复被保存的当前状态，再继续求解原来的问题。

### 基于案例的推理

- 最初是由于目标案例的某些特殊性质联想到记忆中的源案例。但它是粗糙的，不一定正确。
- 在最初的检索结束后，需证实它们之间的可类比性，进一步检索两个类似体的更多的细节，探索它们之间更进一步的可类比性和差异。在这一阶段，事实上，已经初步进行了一些类比映射的工作，只是映射是局部的、不完整的。这个过程结束后，获得的源案例集已经按与目标案例的可类比程度进行了优先级排序。
- 接下来，便进入了类比映射阶段。从源案例集中选择最优的一个源案例，建立它与目标案例之间一致的一一对应。
- 下一步，利用一一对应关系转换源案例的完整的（或部分的）求解方案，从而获得目标案例的完整的（或部分的）求解方案。
- 若目标案例得到部分解答，则把解答的结果加到目标案例的初始描述中，从头开始整个类比过程。若所获得的目标案例的求解方案未能给目标案例以正确的解答，则需解释方案失败的原因，且调用修补过程来修改所获得的方案。系统应该记录失败的原因，以避免以后再出现同样的错误。
- 最后，类比求解的有效性应该得到评价。整个类比过程是递增地进行的。
- 在基于案例的推理中，关心的主要问题如下：
  - 案例表示
  - 分析模型
  - 案例检索
  - 类比映射
  - 类比转换
  - 解释过程
  - 案例修补
  - 类比验证
  - 案例保存

---

## 21.统计学习基本概念和方法，支持向量机线性可分、核函数，结构风险。

### 统计学习的基本概念

- 统计方法是从事物的外在数量上的表现去推断该事物可能的规律性。
- ***书本上**：统计学习（Statistical Leaming）是基于数据构建概率统计模型并运用模型对数据进行预测与分析。*

### 统计学习的方法

- ***书本上**：*
  - *模型的假设空间*
  - *模型选择的准则*
  - *模型学习的算法*
- 统计学习方法
  - 传统方法: 渐近理论，即当样本趋向于无穷多时的统计性质。统计方法主要考虑测试预想的假设和数据模型拟合。它依赖于显式的基本概率模型。 
    - 常见统计方法：
      - 回归分析（多元回归、自回归等）
      - 判别分析（贝叶斯判别、费歇尔判别、非参数判别等）
      - 聚类分析（系统聚类、动态聚类等）
      - 探索性分析（主元分析法、相关分析法等）
  - 模糊集
  - 粗糙集
  - 支持向量机

### 支持向量机

- 支持向量机（support vector machine: **SVM**）是一种二类分类方法，它的基本模型是定义在特征空间上的间隔最大的线性分类器。
- 支持向量机方法是建立在统计学习理论的VC 维理论和结构风险最小原理基础上的。
- 在解决小样本、非线性及高维模式识别中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中。

### 线性可分

- 线性可分与超分离平面
- ![20220325170603-2022-03-25-17-06-05](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220325170603-2022-03-25-17-06-05.png)
- 对于一个线性可分性的样本数据集，其分离超平面通常不止一个。
- SVM模型：分离超平面使得两类样本数据与该分离超平面形成的间隔均为最大。
- SVM只输出样本类别而不输出样本属于某一类别的概率。

### 核函数

- 使用映射函数φ(X)作用于D中所有样本数据，将D映射到高维空间，使得D在高维空间的映像D^′=φ(D)满足线性可分性，即有：
  ` D^'= {(φ(X~1~), y~1~), (φ(X~2~), y~2~), …, (φ(X~N~), y~n~)} `
- ![20220325171055-2022-03-25-17-10-58](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220325171055-2022-03-25-17-10-58.png)

### 机构风险

- 训练样本集：S= {(X~1~, y~1~), (X~2~, y~2~), ..., (X~n~, y~n~)}
- 经验风险
  - ![20220325171901-2022-03-25-17-19-03](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220325171901-2022-03-25-17-19-03.png)
  - 训练样本数足够多时：此时经验风险大体能够代表泛化误差，使用经验风险来代替泛化误差作为模型泛化性能的度量指标；
- 结构风险
  - 训练样本数目较少时：经验风险和泛化误差之间通常会有较大差别，采用结构风险。
  - **结构风险**=**经验风险**+**置信风险**
  - R~srm~(f)= R~emp~(f)+ aλ(f)
  - 其中λ(f)表示模型的复杂程度，αλ(f)可以称为置信风险。
    - 置信风险与模型复杂程度成正比：模型越复杂，则其泛化能力通常越弱，此时置信风险就越大；
    - 置信风险与训练样本数成反比：训练样本较多时，模型泛化能力较强，置信风险则较小
- 对于SVM模型
  - 1、SVM模型优化训练采用的是基于**结构风险最小**的优化策略。
  - 2、通过**对支持向量分类间隔最大化的方式最小化置信风险**，由此获得泛化能力较强的SVM模型。
  - 3、对于训练样本集线性不可分的情形，由于模型训练的经验风险不为0，故此时综合考虑经验风险和置信风险取值，使得经验风险和置信风险的整体取值达到最小，从而获得具有较强泛化能力的SVM模型。
  - 具体的思想是：通过将决策函数候选集:B={f(x,w),w∈Ω}划分为多个子集。 对于每个子集按照VC维度排列，在每个子集中寻找最小经验风险,然后在子集之间折衷考虑经验风险和置信风险之和最小,得到的泛化误差界最小。

---

## 22.强化学习基本概念、模型和方法。

- 强化学习基本概念
  - 强化学习（Reinforcement Leaming，RL），又称激励学习，是从环境到行为映射的学习，以使奖励信号函数值最大。
  - 强化学习不同于监督学习，是**由环境提供的强化信号对产生动作的好坏作出评价**，而不是告诉强化学习系统如何去产生正确的动作。由于外部环境提供的信息很少，学习系统必须靠自身的经历进行学习。通过这种方式，学习系统在行动-评价的环境中获得知识，改进行动方案以适应环境。
- 强化学习模型
  - ![20220325200921-2022-03-25-20-09-23](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220325200921-2022-03-25-20-09-23.png)
  - 每进行一步，智能体根据策略选择一个动作执行，然后感知下一步的状态和即时奖励，通过经验再修改自己的策略。智能体的目标就是最大化长期奖励。
  - 强化学习系统接受环境状态的输人s，根据内部的推理机制，系统输出相应的行为动作a。环境在系统动作作用a下，变迁到新的状态s'。系统接受环境新状态的输人，同时得到环境对于系统的瞬时奖惩反馈r。对强化学习系统来说，其目标是学习一个行为策略π:S→A，使系统选择的动作能够获得环境奖励的累计值最大。
  - 在学习过程中，强化学习技术的基本原理：如果系统某个动作导致环境正的奖励，那么系统以后产生这个动作的趋势便会加强，反之系统产生这个动作的趋势便减弱。这和生理学中的条件反射原理是接近的。
- 强化学习方法
  - 学习自动机
  - 自适应动态程序设计
  - Q-学习
- 学习自动机
  - ![20220325202038-2022-03-25-20-20-40](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220325202038-2022-03-25-20-20-40.png)
  - 这种系统的学习机制包括两个模块：**学习自动机**和**环境**。学习过程是根据环境产生的刺激开始的。自动机根据所接收到的刺激，对环境作出**反应**，环境接收到该反应对其作出**评估**，并向自动机提供**新的刺激**。学习系统根据自动机上次的反应和当前的输入自动地调整其参数。
  - 这里延时模块用于保证上次的反应和当前的刺激同时进人学习系统。
- 自适应动态程序设计
  - 对系统的奖励可以用效用（Utility）函数来表示。在强化学习中，系统可以是主动，也可以是被动的。
    - 被动学习是指系统试图通过自身在不同的环境中的感受来学习其效用函数。
    - 主动学习是指系统能够根据自己学习得到的知识，推出在未知环境中的效用函数。
  - 对于效用函数，可以认为：“一个序列的效用是累积在该序列状态中的奖励之和”。静态效用函数值比较难以得到，因为这需要大量的实验。强化学习的关键是给定训练序列，更新效用值。
  - R(i)是在状态i时的奖励，M~ij~时从状态i到状态j的概率。状态i的效用值U(i):
    - ![20220325210405-2022-03-25-21-04-08](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220325210405-2022-03-25-21-04-08.png)
  - 时差学习，a(0 < a < 1>)为学习率，计算U(i):
    - ![20220325210608-2022-03-25-21-06-11](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220325210608-2022-03-25-21-06-11.png)
  - M~ij~^a^表示状态i执行动作a达到状态j的概率，改进的计算方法：
    - ![20220325210720-2022-03-25-21-07-22](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220325210720-2022-03-25-21-07-22.png)
- Q-学习
  - Q-学习是一种基于时差策略的强化学习，它是指在给定的状态下，在执行完某个动作后期望得到的效用函数，该函数称为动作-值函数。
  - 在Q-学习中，动作-值函数表示为Q(a, i)，它表示在状态i执行动作a的值，也称为Q值。在Q-学习中，使用Q值代替效用值，效用值和Q值之间的关系如下：
    `U(i)= max Q(a, i)`
  - ![20220325211120-2022-03-25-21-11-22](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220325211120-2022-03-25-21-11-22.png)

---

## 23.遗传算法的基本概念，选择、交叉、变异等遗传算子，遗传算法步骤。

### 基础

- 进化计算（evolutionary computation）是研究利用自然进化和适应思想的计算系统。

### 遗传算法的基本概念

- 遗传算法由美国Michigan大学J.H.Holland于60年代提出，**是模仿生物遗传学和自然选择机理，通过人工方式所构造的一类优化搜索算法**，是对生物进化过程进行的一种数学仿真，是进化计算的最重要的形式。
- 基本机理
  - 编码与解码
    - 将问题结构变换为位串形式编码表示的过程叫**编码**；
    - 而相反将位串形式编码表示变换为原问题结构的过程叫**解码**或译码。
    - 把位串形式编码表示叫**染色体**(chromosome)，有时也叫个体。
    - 染色体的每位称为**基因**(gene)。
  - 适应度函数
    - 为了体现染色体的适应能力，引入了对问题中的每一个染色体都能进行度量的函数，叫**适应度函数**(fitness function)。 
    - 适应度函数要有效反映每一个染色体与问题的最优解染色体之间的差距。适应度函数的取值大小与求解问题对象的意义有很大的关系。 
  - 遗传操作

### 选择、交叉、变异等遗传算子

- 选择
  - 根据适者生存原则选择下一代的个体。在选择时，以适应度为选择原则。适应度准则体现了适者生存，不适应者淘汰的自然法则。
  - 给出目标函数f，则f(bi)称为个体bi的适应度。通常选中bi为下一代个体的次数可用下式来计算
- 交叉
  - 将两个个体的部分编码进行交换。
- 变异
  - 改变某位的值。

### 遗传算法步骤

- ***书本上**：*
  ![20220325212253-2022-03-25-21-22-54](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220325212253-2022-03-25-21-22-54.png)
- ***PPT上**：*
  ![20220325212346-2022-03-25-21-23-48](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220325212346-2022-03-25-21-23-48.png)

---

## 24.神经网络的基本概念、基本特征、分类、神经元、激励函数。

### 神经网络的基本概念

- 神经网络（Neural Networks，NN），也称作人工神经网络（Artificial Neural Networks，ANN），或神经计算（Neural Computing，NC），**是对人脑或生物神经网络的抽象和建模，具有从环境学习的能力，以类似生物的交互方式适应环境**。神经网络是智能科学和计算智能的重要部分，以脑科学和认知神经科学的研究成果为基础，拓展智能信息处理的方法，为解决复杂问题和自动控制提供有效的途径。
- 神经网络是由大量处理单元组成的非线性大规模自适应动力系统。

### 神经网络的基本特征

人工神经网络具有四个基本特征：

- （1）非线性
  - 非线性关系是自然界的普遍特性。大脑的智慧就是一种非线性现象。人工神经元处于激活或抑制二种不同的状态，这种行为在数学上表现为一种非线性关系。具有阅值的神经元构成的网络具有更好的性能，可以提高容错性和存储容量。
- （2）
  - 非局限性一个神经网络通常由多个神经元广泛连接而成。一个系统的整体行为不仅取决于单个神经元的特征，而且可能主要由单元之间的相互作用、相互连接所决定。通过单元之间的大量连接模拟大脑的非局限性。联想记忆是非局限性的典型例子。
- （3）非常定性
  - 人工神经网络具有自适应、自组织、自学习能力。神经网络不但处理的信息可以有各种变化，而且在处理信息的同时，非线性动力系统本身也在不断变化。经常采用选代过程描写动力系统的演化过程。
- （4）非凸性
  - 一个系统的演化方向，在一定条件下将取决于某个特定的状态函数。例如能量函数，它的极值相应于系统比较稳定的状态。非凸性是指这种函数有多个极值，故系统具有多个较稳定的平衡态，这将导致系统演化的多样性，

### 神经网络的分类

（来自网络）[常用神经网络的分类-CSND](https://blog.csdn.net/qq_23869697/article/details/79447030?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164821591916780366519734%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=164821591916780366519734&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-79447030.142^v5^pc_search_result_cache,143^v6^register&utm_term=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%88%86%E7%B1%BB&spm=1018.2226.3001.4187)

- BP神经网络
- RBF（径向基）神经网络
- 感知器神经网络
- 线性神经网络
- 自组织神经网络
- 反馈神经网络

### 神经元

- M-P模型
  - ![20220325215507-2022-03-25-21-55-10](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220325215507-2022-03-25-21-55-10.png)
- （来自网络）[人工神经网络中的神经元-知乎](https://zhuanlan.zhihu.com/p/28735794)
  - 神经元的存在形式是一系列参数，并且是可学习的参数。
  - 神经元可以接受不同尺寸的输入，也可以将输入与内部参数进行任意运算/操作（但操作方式应予输入尺寸相匹配）。
  - 神经元的输出大小取决于输入数据的大小和神经元操作方式
  - 神经元在输出前需要apply非线性函数
  - 一个神经元配备一个bias

### 激励函数

- ***书本上**：*
  
  - 从外部环境或其他神经元的输出构成输入向量(x~1~, x~2~, ..., x~n~）^T^，其中x~i~为其他神经元的输出或兴奋水平。连接两个神经元的可调值称为权值或长期记忆。所有和神经元j相连接的权值构成向量W~j= (w~j1~, w~j2~, w~ji~)^T^，其中w~ji~代表处理单元i和j之间的连接权值。通常还加上一个偏置常数θ~j~。
  - 神经元计算过程：`y~i~= f(W~j~^T^ x- θ~j~)`或`y~i~= f(Σw~ji~ x~i~- θ~j~`
  - ![20220327145459-2022-03-27-14-55-00](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220327145459-2022-03-27-14-55-00.png)

- （来自网络）[神经网络 激励函数-CSDn](https://blog.csdn.net/Liang_xj/article/details/82939276)作用是提**供规模化的非线性化能力**，使得神经网络可以任意逼近任何非线性函数，模拟神经元被激发的状态变化。如果不用激励函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合。目前主要有三种常用的激励函数：
  
  - Sigmoid激励函数 ：
    - 也叫 Logistic 函数，用于隐层神经元输出，取值范围为( 0 , 1 )
    - ![20220325215822-2022-03-25-21-58-23](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220325215822-2022-03-25-21-58-23.png)
  - Thah激励函数 ：
    - 读作Hyperbolic Tangent，也称为双切正切函数，取值范围为[ -1, 1 ]。
    - ![20220325215840-2022-03-25-21-58-42](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220325215840-2022-03-25-21-58-42.png)
  - ReLU激励函数（Rectified Linear Unit）：
    - 输入信号 <0 时，输出都是0，>0 的情况下，输出等于输入。
    - ![20220325215851-2022-03-25-21-58-53](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220325215851-2022-03-25-21-58-53.png)

---

## 25.感知器、BP算法。

### 感知机

- ![20220327145540-2022-03-27-14-55-41](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220327145540-2022-03-27-14-55-41.png)
- 外界信号s经过加权后输入最后一个单元，若不小于偏置，则输出为1，否则为-1。
- 设网络注入模式向量为s^k^= (s~1~^k^, s~2~^k^, ..., s~n~^k^)^T^，对应的期望输出为d^k^；由输入自输出的权值向量为W= (w~1~, w~2~, ..., w~n~)^T^。
- ![20220327150135-2022-03-27-15-01-36](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220327150135-2022-03-27-15-01-36.png)

### BP算法

- 前反馈神经网络基础
  
  - 一个前馈神经网络可以定义为无圈的有向图N=(V, E, W)，其中V={0, 1, ..., n}为神经元集合，E∈V*V为连接权值集合，W:E→R为对每一连接(i, j)∈ E赋予一实值的权重w~ij~。对神经元i∈V定义它的投射域为P~i~= (j: j∈V, (j, i)∈ E}，即表示单元i的输出经加权后直接作为其净输入的一部分的神经单元；同样地定义神经元的接受域为R~i~= (j: j∈V, (i, j)∈E}，即表示其输出经加权后直接作为神经元i的净输入的一部分的神经单元。特别地，对分层前馈神经网络来说，**每个神经元的接受域和投射域分别是其所在层的前一层神经元和后一层神经元**（若它们存在）。神经元集V可以被分成**无接受域的输入节点集V~1~，无投射域的输出节点集V~0~和隐节点集V~H~**。一般地假设一个特殊的偏置节点（这里其标号为0），其输出恒为+1，它和输入节点除外的所有节点相连。
  - 前馈神经网络的输入单元从外部环境中接收信号，经处理将输出信号加权后传给其投射域中的神经元，网络中的隐单元或输出单元i从其接受域中接受净输入net~i~= ∑w~ij~ y~i~，（其中y~j~表示单元j的输出），然后向它的投射域P，发送输出信号y~i~= f(net~i~)，f可以为任意的可微函数，一般常用的为f(x)= 1/ (1+e-x)。上述过程一直持续到所有的输出单元得到输出为止，它们的输出作为网络的输出。

- 误差反向传播算法（基本的BP算法）
  
  - ![20220327155601-2022-03-27-15-56-02](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220327155601-2022-03-27-15-56-02.png)

---

## 26.深度学习的基本概念、模型和方法。

### 深度学习的基本概念

- 模拟人脑的层级抽象结构，通过无监督的方式分析大规模数据，发掘大数据中蕴藏的有价值信息。

### 深度学习的模型

### 深度学习的方法

---

## 27.专家系统的基本概念、特点、基本结构和评价方法。

### 专家系统的基本概念

- 专家系统是一个含有大量的某个领域专家水平的知识与经验智能计算机程序系统，能够利用人类专家的知识和解决问题的方法来处理该领域问题。
- 简而言之，专家系统是一种模拟人类专家解决领域问题的计算机程序系统。
- ***书本上**：专家系统（Expert System）是一类具有专门知识和经验的计算机智能程序系统，通过对人类专家问题求解能力的建模，采用人工智能中的知识表示和知识推理技术来模拟通常由专家才能解决的复杂问题，达到具有与专家同等解决问题的水平。*

### 专家系统的特点

- ⑴ 启发性：专家系统能运用专家的知识与经验进行推理、判断和决策。
- ⑵ 透明性：专家系统能够解释本身的推理过程和回答用户提出的问题，以便让用户能够了解推理过程，提高对专家系统的信赖感。
- ⑶ 灵活性：专家系统能不断地增长知识，修改原有知识，不断更新。
- 4）交互性。专家系统一般采用交互方式进行人机通信，这种交互性既有利于系统从专家那里获取知识，又便于用户在求解问题时输人条件或事实。
- 5）实用性。专家系统是根据具体应用领域的问题开发的，针对性强，具有非常良好的实用性。
- 6）易推广。专家系统使人类专家的领域知识突破了时间和空间的限制，专家系统的知识库可以永久保存，并可复制任意多的副本或在网上供不同地区或部门的人们使用，从而使专家的知识和技能更易于推广和传播。

### 专家系统的基本结构

- 专家系统通常由人机交互界面、知识库、推理机、解释器、综合数据库和知识获取6个部分构成。
- ![20220327163951-2022-03-27-16-39-52](http://lengyuewusheng-blog.oss-cn-beijing.aliyuncs.com/blog/20220327163951-2022-03-27-16-39-52.png)

### 专家系统的评价方法

（知识系统的评价方法）

- ⑴ 计算 主要包括运行速度、存储空间、可扩展性、可移植性和是否易于与其它软件集成。
- ⑵ 感观 主要包括易用性、可理解性、自然性和是否提供联机帮助和解释。
- ⑶ 性能 主要包括能力范围、得出非正确解答的比率、时间和资金的节省等。

---

## 28.典型的专家系统工具。

- ⑴ 骨架（外壳）型开发工具
  - 专家系统一般都有推理机和知识库两部分，而规则集存于知识库内。在一个理想的专家系统中，推理机完全独立于求解问题领域。系统功能上的完善或改变，只依赖于规则集的完善和改变。由此，借用以前开发好的专家系统，将描述领域知识的规则从原系统中“挖掉”，只保留其独立于问题领域知识的推理机部分，这样形成的工具称为骨架型工具。这类工具因其控制策略是预先给定的，使用起来很方便，用户只须将具体领域的知识明确地表示成为一些规则就可以了。
  - 因其程序的主要骨架是固定的，除了规则以外，用户不可改变任何东西，因而骨架型工具存在一些有待解决的问题，影响它的广泛应用。
- ⑵ 语言型开发工具
  - 语言型工具提供给用户的是建立专家系统所需要的基本机制，其控制策略也不固定于一种或几种形式，用户可以通过一定手段来影响其控制策略。因此，语言型工具的结构变化范围广泛，表示灵活，所适应的范围要比骨架型工具广泛得多。
  - LISP语言和PROLOG语言
- ⑶ 构造辅助工具
  - 系统构造辅助工具由一些程序模块组成，有些程序能帮助获得和表达领域专家的知识，有些程序能帮助设计正在构造的专家系统的结构。它主要分两类，一种是设计辅助工具，另一种是知识获取辅助工具。
- ⑷ 支撑环境
  - 支撑设施是指帮助进行程序设计的工具，它常被作为知识工程语言的一部分。工具支撑环境仅是一个附带的软件包，以便使用户界面更友好。它包括四个典型组件：调试辅助工具、输入输出设施、解释设施和知识库编辑器。

---

## 29.专家系统构建的步骤和方法。

1. 需求分析
2. 系统设计
3. *知识库构建*（设计初始知识库）
4. *系统开发*（原型机的开发与试验）
5. *系统测试*（知识库的改进与归纳）
